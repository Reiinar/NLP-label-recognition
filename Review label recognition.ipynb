{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(review) for review in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word_index is a dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "# We reverse it, mapping integer indices to words\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_review(review_index):\n",
    "    review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[review_index]])\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? as a big fan of the original film it's hard to watch this show the ? set ? and ? ? sets rob any style from this remake the mood is never there instead it has the look and feel of so many television movies of the seventies crenna is not a bad choice as walter ? but his snappy wardrobe and ? apartment don't fit the mood of the original or make him an interesting character he does his best to make it work but samantha ? is a really bad choice the english accent and california looks can't hold a candle to barbara ? ? voice and sex appeal lee j ? tries ? to fashion barton ? but even his performance is just gruff without style br br it feels like the tv movie it was and again reminds me of what a remarkable film the original still is\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "give_review(24999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48570\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4df6b0ca7f496993f42ebe73bbfd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31307cb43ddb4d6faef45ac3cb1451a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in tqdm(enumerate(sequences)):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# Create a tokenizer, configured to only take\n",
    "# into account the top-1000 most common words\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "# This builds the word index\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# This turns strings into lists of integer indices.\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9996 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "reviews = [give_review(i) for i in range(len(train_data))]\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "\n",
    "# This is how you can recover the word index that was computed\n",
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set_x = x_train[:10000]\n",
    "the_rest_train_set_x = x_train[10000:]\n",
    "\n",
    "val_set_y = y_train[:10000]\n",
    "the_rest_train_set_y = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 3s 216us/step - loss: 0.5106 - binary_accuracy: 0.7811 - val_loss: 0.3901 - val_binary_accuracy: 0.8600\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 2s 139us/step - loss: 0.3024 - binary_accuracy: 0.9023 - val_loss: 0.3130 - val_binary_accuracy: 0.8818\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 2s 141us/step - loss: 0.2178 - binary_accuracy: 0.9323 - val_loss: 0.2854 - val_binary_accuracy: 0.8870\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 2s 136us/step - loss: 0.1713 - binary_accuracy: 0.9447 - val_loss: 0.2819 - val_binary_accuracy: 0.8862\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 2s 136us/step - loss: 0.1405 - binary_accuracy: 0.9539 - val_loss: 0.2914 - val_binary_accuracy: 0.8863\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 2s 137us/step - loss: 0.1149 - binary_accuracy: 0.9643 - val_loss: 0.3001 - val_binary_accuracy: 0.8860\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 2s 151us/step - loss: 0.0962 - binary_accuracy: 0.9700 - val_loss: 0.3332 - val_binary_accuracy: 0.8759\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 2s 152us/step - loss: 0.0775 - binary_accuracy: 0.9777 - val_loss: 0.3392 - val_binary_accuracy: 0.8807\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 2s 142us/step - loss: 0.0608 - binary_accuracy: 0.9842 - val_loss: 0.3641 - val_binary_accuracy: 0.8777\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 2s 152us/step - loss: 0.0541 - binary_accuracy: 0.9847 - val_loss: 0.3852 - val_binary_accuracy: 0.8757\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 2s 144us/step - loss: 0.0393 - binary_accuracy: 0.9913 - val_loss: 0.4234 - val_binary_accuracy: 0.8716\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 2s 140us/step - loss: 0.0347 - binary_accuracy: 0.9925 - val_loss: 0.4425 - val_binary_accuracy: 0.8736\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 2s 137us/step - loss: 0.0268 - binary_accuracy: 0.9947 - val_loss: 0.4744 - val_binary_accuracy: 0.8723\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 2s 140us/step - loss: 0.0225 - binary_accuracy: 0.9953 - val_loss: 0.5054 - val_binary_accuracy: 0.8714\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 2s 143us/step - loss: 0.0126 - binary_accuracy: 0.9989 - val_loss: 0.5417 - val_binary_accuracy: 0.8713\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 2s 137us/step - loss: 0.0135 - binary_accuracy: 0.9977 - val_loss: 0.6028 - val_binary_accuracy: 0.8675\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 2s 147us/step - loss: 0.0114 - binary_accuracy: 0.9986 - val_loss: 0.6064 - val_binary_accuracy: 0.8683\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 2s 136us/step - loss: 0.0057 - binary_accuracy: 0.9997 - val_loss: 0.6731 - val_binary_accuracy: 0.8644\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 2s 135us/step - loss: 0.0060 - binary_accuracy: 0.9995 - val_loss: 0.6760 - val_binary_accuracy: 0.8668\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 2s 146us/step - loss: 0.0062 - binary_accuracy: 0.9989 - val_loss: 0.7092 - val_binary_accuracy: 0.8660\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(the_rest_train_set_x,\n",
    "                    the_rest_train_set_y,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(val_set_x, val_set_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_binary_accuracy', 'loss', 'binary_accuracy'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32499700770378115, 0.8728]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In 88% cases our model can predict wheter the review is good or bad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
